from transformers import pipeline, AutoTokenizer
from transformers import GenerationConfig, AutoModelForCausalLM
import time
from torch.utils.data import DataLoader

import os

from dataclasses import dataclass, field
from typing import Optional
from accelerate import Accelerator
import json
import torch
from datasets import load_dataset, Dataset
from tqdm import tqdm
from transformers import AutoTokenizer, HfArgumentParser, pipeline, DataCollatorForSeq2Seq
from transformers import DataCollatorForLanguageModeling, PreTrainedModel, PreTrainedTokenizerBase, TrainerCallback
from typing import Any, Dict, List, Optional, Tuple, Union

import numpy as np
import pandas as pd
tqdm.pandas()


@dataclass
class ScriptArguments:
    """
    The arguments for the DPO training script.
    """
    model_name_or_path: Optional[str] = field(
        default="/home/xiongwei/gshf_gold_test/LMFlow_RAFT_Dev/output_models/online_dpo_strong_ref/iter2/tmp/checkpoint-pymodel3500",
        metadata={"help": "the location of the SFT model name or path"},
    )
    dataset_name_or_path: Optional[str] = field(
        default="/home/xiongwei/rm_study/LMFlow/data/no_share_gpt_hh/offline/split_3iters/iter3/rlhf_prompt/split_prompt.json",
        metadata={"help": "the location of the dataset name or path"},
    )
    output_dir: Optional[str] = field(
        default="/home/xiongwei/gshf_gold_test/LMFlow_RAFT_Dev/output_models/online_dpo_strong_ref/iter3/iter3_k1_by_current_policy.json",
        metadata={"help": "the location of the output file"},
    )
    gen_batch_size: Optional[int] = field(
        default=24,
        metadata={"help": "the batch size for inference"},
    )
    K: Optional[int] = field(
        default=1,
        metadata={"help": "the number of generations per prompt"},
    )
    max_new_tokens: Optional[int] = field(
        default=400,
        metadata={"help": "the maximum length of the new tokens"},
    )


@torch.no_grad()
def generate(
    model: PreTrainedModel,
    dataloader: DataLoader,
    tokenizer: PreTrainedTokenizerBase,
    accelerator: Accelerator,
    seed: int,
    **generation_kwargs,
) -> Tuple[List[str], List[str]]:
    """Generate samples using model using specified generation_kwargs.

    Args:
        model: the model used to score samples.
        dataloader: the dataloader containing batches of elements from the generated dataset.
        tokenizer: the tokenizer.
        accelerator: the accelerator object.

    Returns:
        all_prompts: the prompts.
        all_responses: the responses generated by the model.
    """

    all_predictions = []
    all_prompts = []
    pbar = tqdm(total=len(dataloader),
                disable=not accelerator.is_local_main_process)

    torch.manual_seed(seed=seed)
    torch.cuda.manual_seed_all(seed=seed)
    np.random.seed(seed=seed)

    for batch in dataloader:
        sequence_length = batch["input_ids"].shape[1]

        all_tokens = accelerator.unwrap_model(model).generate(
            batch["input_ids"],
            #attention_mask=batch["attention_mask"],
            **generation_kwargs,
        )

        generated_tokens = all_tokens[:, sequence_length:]
        prompt_tokens = all_tokens[:, :sequence_length]

        generated_tokens = accelerator.pad_across_processes(
            generated_tokens, dim=1, pad_index=tokenizer.pad_token_id)
        prompt_tokens = accelerator.pad_across_processes(
            prompt_tokens, dim=1, pad_index=tokenizer.pad_token_id)

        generated_tokens = accelerator.gather(generated_tokens)
        generated_tokens = generated_tokens.cpu()
        prompt_tokens = accelerator.gather(prompt_tokens)
        prompt_tokens = prompt_tokens.cpu()

        if isinstance(generated_tokens, tuple):
            generated_tokens = generated_tokens[0]
            prompt_tokens = prompt_tokens[0]

        all_predictions.extend(generated_tokens)
        all_prompts.extend(prompt_tokens)
        pbar.update(1)

    accelerator.wait_for_everyone()

    all_predictions = tokenizer.batch_decode(
        all_predictions, skip_special_tokens=True)
    all_prompts = tokenizer.batch_decode(all_prompts, skip_special_tokens=True)

    return all_prompts, all_predictions


accelerator = Accelerator()
parser = HfArgumentParser(ScriptArguments)
script_args = parser.parse_args_into_dataclasses()[0]


prompt_dir = script_args.dataset_name_or_path
model_name = script_args.model_name_or_path
output_dir = script_args.output_dir
K = script_args.K

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.pad_token_id = tokenizer.eos_token_id
tokenizer.padding_side = "left"

generation_kwargs = {
    "top_k": 0.0,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
    "temperature": 1.0,
    "num_return_sequences": script_args.K,
    "max_new_tokens": script_args.max_new_tokens
}


def _clean_text(text):
    return text
    if len(text) == 0:
        return text
    stext = [x for x in text.split("###Human") if x]

    return stext[0].strip().replace("#", "").replace("<s>", "").replace("</s>", "")




ds = load_dataset("json", data_files=prompt_dir,
                  split="train", field="instances")#.select(range(2048))

#print(ds)

#all_prompts = [sample['text'] for sample in ds]
device = accelerator.device

model = AutoModelForCausalLM.from_pretrained(
    model_name, torch_dtype=torch.bfloat16, trust_remote_code=True
)

model = model.to(device)
model.gradient_checkpointing_disable()
model.config.use_cache = True
local_rank = Accelerator().local_process_index


def tokenize_fn(sample):
    sample['text'] = tokenizer.apply_chat_template(
        sample['prompt'], tokenize=False).replace(tokenizer.bos_token, "")
    return sample
    #model_inputs = tokenizer(sample["text"])
    #return {
    #    **model_inputs,
    #}


def tokenize_fn2(sample):
    
    model_inputs = tokenizer(sample["text"])
    return {
        **model_inputs,
     }

ds = ds.map(tokenize_fn, batched=False, remove_columns=list(ds.features))
ds = ds.map(tokenize_fn2, batched=True, remove_columns=list(ds.features))
print(ds)
print(ds[0])

data_collator = DataCollatorForSeq2Seq(tokenizer, pad_to_multiple_of=1)
dataloader = DataLoader(ds, batch_size=script_args.gen_batch_size,
                        shuffle=False, collate_fn=data_collator)

#optimizer = torch.optim.SGD(
#    filter(lambda p: p.requires_grad, model.parameters()), lr=0.0001)
model, dataloader = accelerator.prepare(model, dataloader)

prompts, responses = generate(
    model, dataloader, tokenizer, accelerator, seed=42, **generation_kwargs)

generated_dataset = Dataset.from_dict(
    {"prompt": prompts, "response": responses})

assert len(prompts) == len(responses)

gathered_data = []
prompts_set = {}


# In case of repeated prompts, we merge their outputs
for i in range(len(prompts)):
    if prompts[i] not in prompts_set:
        tmp_data = {"prompt": prompts[i], "responses": [_clean_text(responses[i])]}
        gathered_data.append(tmp_data)
        prompts_set[prompts[i]] = len(gathered_data)
    else:
        gathered_data[prompts_set[prompts[i]] -
                      1]["responses"].append(_clean_text(responses[i]))

output_eval_dataset = {}
output_eval_dataset['type'] = 'text_only'
output_eval_dataset['instances'] = gathered_data
print("I collect ", len(gathered_data), "samples")


if local_rank == 0:
    with open(output_dir, 'w', encoding='utf8') as f:
        json.dump(output_eval_dataset, f, ensure_ascii=False)
